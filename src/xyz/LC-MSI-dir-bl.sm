

machine(MachineType:Directory, "Directory protocol")
    :
  DirectoryMemory * directory;
  CacheMemory * cacheMemory;
  XYZCacheMemory *xyzCacheMemory;
  Cycles toMemLatency := 1;
  int sum_prv_cores := 0;
  int sum_prv_capacity := 0;
MessageBuffer *forwardToCache, network = "To", virtual_network = "1",
                               vnet_type = "forward";
MessageBuffer *responseToCache, network = "To", virtual_network = "2",
                                vnet_type = "response";
MessageBuffer *requestFromCache, network = "From", virtual_network = "0",
                                 vnet_type = "request";
MessageBuffer *responseFromCache, network = "From", virtual_network = "2",
                                  vnet_type = "response";
// NOTE: peek from requestFromOrdered should only be processed if the request CAN be processed...
// NOTE: the request sent to requestToOrder will be ordered in the PerfectTDMSwitch and will be serviced back later
// NOTE: all memory request should be ordered even in the case of ZIV + ROC because a later reqeust can 
// Occupy a CRE
MessageBuffer *requestToOrder, network = "To", virtual_network = "3",
                                  vnet_type = "request";
MessageBuffer *requestFromOrdered, network = "From", virtual_network = "3",
                                  vnet_type = "request";
MessageBuffer *requestToMemory;
MessageBuffer *responseFromMemory;
{
  state_declaration(State, desc="Directory states",
                  default="Directory_State_I") {
      // Stable states.
      // NOTE: These are "cache-centric" states like in Sorin et al.
      // However, The access permissions are memory-centric.
      I, AccessPermission:Read_Write,  desc="Invalid in the caches.";
      S, AccessPermission:Read_Only,   desc="At least one cache has the blk";
      M, AccessPermission:Invalid,     desc="A cache has the block in M";
      LLCOnly, AccessPermission:Read_Write, desc="In LLC only";

      // Transient states
      S_D, AccessPermission:Busy,      desc="Moving to S, but need data";

      // Waiting for data from memory
      S_m, AccessPermission:Read_Write, desc="In S waiting for mem";
      M_m, AccessPermission:Read_Write, desc="Moving to M waiting for mem";

      // Waiting for write-ack from memory
      MI_m, AccessPermission:Busy,       desc="Moving to I waiting for ack";
      SS_m, AccessPermission:Busy,       desc="Moving to I waiting for ack";
  }

  enumeration(Event, desc="Directory events") {
        // Data requests from the cache
        GetS,         desc="Request for read-only data from cache";
        GetM,         desc="Request for read-write data from cache";
        Replacement,  desc="Triggered when block is chosen as victim";

        // Writeback requests from the cache
        PutSNotLast,  desc="PutS and the block has other sharers";
        PutSLast,     desc="PutS and the block has no other sharers";
        PutMOwner,    desc="Dirty data writeback from the owner";
        PutMNonOwner, desc="Dirty data writeback from non-owner";

        // Cache responses
        Data,         desc="Response to fwd request with data";

        // From Memory
        MemData,      desc="Data from memory";
        MemAck,       desc="Ack from memory that write is complete";

        // Ordering
        Order,        desc="Order what every request";
    }

    // main == false?
    structure(Entry, desc="...", interface="AbstractCacheEntry", main="true") {
        State DirState,         desc="Directory state";
        NetDest Sharers,        desc="Sharers for this block";
        NetDest Owner,          desc="Owner of this block";
    }
    /*
    structure(TagEntry, desc="...", interface="AbstractCacheEntry") {
        State DirState,         desc="Directory state";
        NetDest Sharers,        desc="Sharers for this block";
        NetDest Owner,          desc="Owner of this block";
    } */
    // For some reason it does not like the default case, we must add interface to make it pass by pointers
    structure(TBE, desc="MSHR like requests", main="false") {
        Addr addr;
        State TBEState,         desc="State of block";
        DataBlock DataBlk,      desc="Data for the block. Needed for MI_A";
        int AcksOutstanding, default=0, desc="Number of acks left to receive.";
    }
    // NOTE: the tag in the XYZCacheMemory is implemented as a fully associative array
    structure(DataEntry, desc="...", interface="AbstractCacheEntry", main="false") {
        State DirState,         desc="Directory state";
        NetDest Sharers,        desc="Sharers for this block";
        NetDest Owner,          desc="Owner of this block";
        DataBlock DataBlk,       desc="Data in the block";
        bool isDirty,           desc="Specify whether we really need to perform memory transaction";
    }
    // This should be indexed by the set address
    structure(TBETable, external="yes") {
        TBE lookup(Addr);
        void allocate(Addr);
        void deallocate(Addr);
        bool isPresent(Addr);
    }
    TBETable TBEs, template="<Directory_TBE>", constructor="m_number_of_TBEs";

    // The state can only be in the tag cache or the TBE
    State xyzGetState(TBE tbe, Entry cache_entry, Addr addr) {
        if(is_valid(tbe)) { return tbe.TBEState; }
        else if(is_valid(cache_entry)) { return cache_entry.DirState; }
        else { return State:I; }
    }

        
    void xyzSetState(TBE tbe, Entry cache_entry, Addr addr, State state) {
        if (is_valid(tbe)) { tbe.TBEState := state; }
        if (is_valid(cache_entry)) { cache_entry.DirState := state; }
    }

    Tick clockEdge();
    Entry getDirectoryEntry(Addr addr), return_by_pointer = "yes" {
      Entry dir_entry := static_cast(Entry, "pointer", directory[addr]);
      if (is_invalid(dir_entry)) {
          // This first time we see this address allocate an entry for it.
          dir_entry := static_cast(Entry, "pointer",
                                  directory.allocate(addr, new Entry));
      }
      return dir_entry;
    }
    
    
    DataEntry getDataEntry(Addr address), return_by_pointer="yes" {
      // return static_cast(DataEntry, "pointer", xyzCacheMemory.lookup(address));
      DataEntry de := static_cast(DataEntry, "pointer", xyzCacheMemory[address]);
      return de;
    }


    Tick cyclesToTicks(Cycles c);
    Cycles curCycle();
    Cycles ticksToCycles(Tick t);
    // RubySlicc_Util.hh
    int getCurSlotOwnerHelper(Cycles c, int slot_width, int nCore);
    int getCurSlotOwner() {

        int owner := getCurSlotOwnerHelper(curCycle(), 128, 4);
        return owner;
    }
    
  State getState(TBE tbe, Entry cache_entry, Addr addr) {
      DataEntry de := getDataEntry(addr);
      if (directory.isPresent(addr)) {
          Entry e := getDirectoryEntry(addr);
          if(e.DirState == State:I) {
              if(is_valid(de)) {
                  return State:LLCOnly;
              }
          }
          return e.DirState;
      } else if(is_valid(de)) {
          // This only happens if it is not in the TagCache
          return State:LLCOnly;
      } else {
          return State:I;
      }
  }


  void setState(TBE tbe, Entry cache_entry, Addr addr, State state) {
      if (directory.isPresent(addr)) {
          if (state == State:M) {
              DPRINTF(RubySlicc, "Owner %s\n", getDirectoryEntry(addr).Owner);
              assert(getDirectoryEntry(addr).Owner.count() == 1);
              assert(getDirectoryEntry(addr).Sharers.count() == 0);
          }
          getDirectoryEntry(addr).DirState := state;
          if (state == State:I)  {
              assert(getDirectoryEntry(addr).Owner.count() == 0);
              assert(getDirectoryEntry(addr).Sharers.count() == 0);
          }
      }
  }

  AccessPermission getAccessPermission(Addr addr) {
      if (directory.isPresent(addr)) {
          Entry e := getDirectoryEntry(addr);
          return Directory_State_to_permission(e.DirState);
      } else  {
          return AccessPermission:NotPresent;
      }
  }
  void setAccessPermission(Entry cache_entry, Addr addr, State state) {
      if (directory.isPresent(addr)) {
          Entry e := getDirectoryEntry(addr);
          e.changePermission(Directory_State_to_permission(state));
      }
  }

  void functionalRead(Addr addr, Packet *pkt) {
    functionalMemoryRead(pkt);
  }

  int functionalWrite(Addr addr, Packet *pkt) {
    if (functionalMemoryWrite(pkt)) {
        return 1;
    } else {
        return 0;
    }
  }
  out_port(forward_out, RequestMsg, forwardToCache);

  out_port(response_out, ResponseMsg, responseToCache);
  out_port(memQueue_out, MemoryMsg, requestToMemory);
  out_port(request_out_order, RequestMsg, requestToOrder);
  

  in_port(memQueue_in, MemoryMsg, responseFromMemory) {
      if (memQueue_in.isReady(clockEdge())) {
          peek(memQueue_in, MemoryMsg) {
              // // Just check that in the actions
              // TagEntry te := getTagEntry(in_msg.addr);
              Entry te := getDirectoryEntry(in_msg.addr);
              // DataEntry de := getDataEntry(in_msg.addr);
              Addr set_idx := xyzCacheMemory.addressToCacheSet(in_msg.addr);
              // TODO: with something else
              TBE tbe := TBEs[in_msg.addr];
              if (in_msg.Type == MemoryRequestType:MEMORY_READ) {
                  trigger(Event:MemData, in_msg.addr, te, tbe);
              } else if (in_msg.Type == MemoryRequestType:MEMORY_WB) {
                  trigger(Event:MemAck, in_msg.addr, te, tbe);
              } else {
                  error("Invalid message");
              }
          }
      }
  }

  in_port(response_in, ResponseMsg, responseFromCache) {
      if (response_in.isReady(clockEdge())) {
          peek(response_in, ResponseMsg) {
              
              Entry te := getDirectoryEntry(in_msg.addr);
              // TagEntry te := getTagEntry(in_msg.addr);
              // DataEntry de := getDataEntry(in_msg.addr);
              Addr set_idx := xyzCacheMemory.addressToCacheSet(in_msg.addr);
              TBE tbe := TBEs[set_idx];
              if (in_msg.Type == CoherenceResponseType:Data) {
                  trigger(Event:Data, in_msg.addr, te, tbe);
              } else {
                  error("Unexpected message type.");
              }
          }
      }
  }

  in_port(request_in, RequestMsg, requestFromCache) {
      if (request_in.isReady(clockEdge())) {
          peek(request_in, RequestMsg) {
              Entry e := getDirectoryEntry(in_msg.addr);
              // TagEntry te := getTagEntry(in_msg.addr);
              Addr set_idx := xyzCacheMemory.addressToCacheSet(in_msg.addr);
              // TBE tbe := TBEs[0];

              TBE tbe := TBEs[in_msg.addr];
              // The tag cache must have enough space, the directory cache however does not ne
              if(is_valid(tbe)) {
                  // Requesting on a cache line that needs to be ordered
                  trigger(Event:Order, in_msg.addr, e, tbe);
              } else if (in_msg.Type == CoherenceRequestType:GetS) {

                  trigger(Event:GetS, in_msg.addr, e, tbe);
              } else if (in_msg.Type == CoherenceRequestType:GetM) {
                  trigger(Event:GetM, in_msg.addr, e, tbe);
              } else if (in_msg.Type == CoherenceRequestType:PutS) {
                  assert(is_valid(e));
                  // If there is only a single sharer (i.e., the requestor)
                  if (e.Sharers.count() == 1) {
                      assert(e.Sharers.isElement(in_msg.Requestor));
                      trigger(Event:PutSLast, in_msg.addr, e, tbe);
                  } else {
                      trigger(Event:PutSNotLast, in_msg.addr, e, tbe);
                  }
              } else if (in_msg.Type == CoherenceRequestType:PutM) {
                  assert(is_valid(e));
                  if (e.Owner.isElement(in_msg.Requestor)) {
                      trigger(Event:PutMOwner, in_msg.addr, e, tbe);
                  } else {
                      trigger(Event:PutMNonOwner, in_msg.addr, e, tbe);
                  }
              } else {
                  error("Unexpected message type.");
              }
          }
      }
  }
  in_port(request_in_order, RequestMsg, requestFromOrdered) {
      if(request_in_order.isReady(clockEdge())) {
          peek(request_in_order, RequestMsg) {
              Entry e := getDirectoryEntry(in_msg.addr);
              // DataEntry de := getDataEntry(in_msg.addr);
              Addr set_idx := xyzCacheMemory.addressToCacheSet(in_msg.addr);
              // TBE tbe := TBEs[0];

              TBE tbe := TBEs[in_msg.addr];

              Entry te := getDirectoryEntry(in_msg.addr);
              // TagEntry te := getTagEntry(in_msg.addr);
          }
      }
  }

  action(allocteROCTBE, 'aTBE', desc="Allocate tbe entry for RoC for current pending request") {
      peek(request_in, RequestMsg) {  
        Addr set_idx := xyzCacheMemory.addressToCacheSet(in_msg.addr);
        // TBEs.allocate(set_idx);
        // MSHREntry tbe := TBEs[set_idx];
        // tbe.addr := in_msg.addr;
      }
  }

  action(fwdToOrder, "fwdOrd", desc="Forward a request to order for RoC") {
      peek(request_in, RequestMsg) {
          enqueue(request_out_order, RequestMsg) {
              // forward as is, we will need it later
              out_msg := in_msg;
          }
      }
  }

  action(sendMemRead, "r", desc="Send a memory read request") {
        peek(request_in, RequestMsg) {
            // Send request through special memory request queue. At some
            // point the response will be on the memory response queue.
            // Like enqueue, this takes a latency for the request.
            enqueue(memQueue_out, MemoryMsg, toMemLatency) {
                out_msg.addr := address;
                out_msg.Type := MemoryRequestType:MEMORY_READ;
                out_msg.Sender := in_msg.Requestor;
                out_msg.MessageSize := MessageSizeType:Request_Control;
                out_msg.Len := 0;

                DPRINTF(RubySlicc, "Reading memory for %#x\n", address);

                DPRINTF(RubySlicc, "Out Msg: %s\n", out_msg);
            }
        }
    }

    action(sendDataToMem, "w", desc="Write data to memory") {
        peek(request_in, RequestMsg) {
            DPRINTF(RubySlicc, "Writing memory for %#x\n", address);
            DPRINTF(RubySlicc, "Writing %s\n", in_msg.DataBlk);
            enqueue(memQueue_out, MemoryMsg, toMemLatency) {
                out_msg.addr := address;
                out_msg.Type := MemoryRequestType:MEMORY_WB;
                out_msg.Sender := in_msg.Requestor;
                out_msg.MessageSize := MessageSizeType:Writeback_Data;
                out_msg.DataBlk := in_msg.DataBlk;
                out_msg.Len := 0;
            }
        }
    }

    action(sendRespDataToMem, "rw", desc="Write data to memory from resp") {
        peek(response_in, ResponseMsg) {
            DPRINTF(RubySlicc, "Writing memory for %#x\n", address);
            DPRINTF(RubySlicc, "Writing %s\n", in_msg.DataBlk);
            enqueue(memQueue_out, MemoryMsg, toMemLatency) {
                out_msg.addr := address;
                out_msg.Type := MemoryRequestType:MEMORY_WB;
                out_msg.Sender := in_msg.Sender;
                out_msg.MessageSize := MessageSizeType:Writeback_Data;
                out_msg.DataBlk := in_msg.DataBlk;
                out_msg.Len := 0;
            }
        }
    }
  action(addReqToSharers, "aS", desc="Add requestor to sharer list") {
      peek(request_in, RequestMsg) {
          sum_prv_cores := sum_prv_cores + 1;
          getDirectoryEntry(address).Sharers.add(in_msg.Requestor);
      }
  }

  action(setOwner, "sO", desc="Set the owner") {
      peek(request_in, RequestMsg) {
          Entry e := getDirectoryEntry(address);
          sum_prv_cores := sum_prv_cores - e.Sharers.count() + 1;
          e.Owner.add(in_msg.Requestor);
      }
  }

  action(addOwnerToSharers, "oS", desc="Add the owner to sharers") {
      Entry e := getDirectoryEntry(address);
      assert(e.Owner.count() == 1);
      e.Sharers.addNetDest(e.Owner);
      sum_prv_cores := sum_prv_cores - 1 + e.Sharers.count();
  }

  action(removeReqFromSharers, "rS", desc="Remove requestor from sharers") {
      peek(request_in, RequestMsg) {
          getDirectoryEntry(address).Sharers.remove(in_msg.Requestor);
          sum_prv_cores := sum_prv_cores - 1;
      }
  }

  action(clearSharers, "cS", desc="Clear the sharer list") {
      Entry e := getDirectoryEntry(address);
      sum_prv_cores := sum_prv_cores - e.Sharers.count();
      e.Sharers.clear();
  }

  action(clearOwner, "cO", desc="Clear the owner") {
      sum_prv_cores := sum_prv_cores - 1;
      getDirectoryEntry(address).Owner.clear();
  }

  action(sendInvToSharers, "i", desc="Send invalidate to all sharers") {
      // broadcast
      peek(request_in, RequestMsg) {
          enqueue(forward_out, RequestMsg, 1) {
              out_msg.addr := address;
              out_msg.Type := CoherenceRequestType:Inv;
              out_msg.Requestor := in_msg.Requestor;
              out_msg.Destination := getDirectoryEntry(address).Sharers;
              out_msg.MessageSize := MessageSizeType:Control;
          }
      }
  }

  action(sendFwdGetS, "fS", desc="Send forward getS to owner") {
      assert(getDirectoryEntry(address).Owner.count() == 1);
      peek(request_in, RequestMsg) {
          enqueue(forward_out, RequestMsg, 1) {
              out_msg.addr := address;
              out_msg.Type := CoherenceRequestType:GetS;
              out_msg.Requestor := in_msg.Requestor;
              out_msg.Destination := getDirectoryEntry(address).Owner;
              out_msg.MessageSize := MessageSizeType:Control;
          }
      }
  }

  action(sendFwdGetM, "fM", desc="Send forward getM to owner") {
      assert(getDirectoryEntry(address).Owner.count() == 1);
      peek(request_in, RequestMsg) {
          enqueue(forward_out, RequestMsg, 1) {
              out_msg.addr := address;
              out_msg.Type := CoherenceRequestType:GetM;
              out_msg.Requestor := in_msg.Requestor;
              out_msg.Destination := getDirectoryEntry(address).Owner;
              out_msg.MessageSize := MessageSizeType:Control;
          }
      }
  }
  action(sendDataToReq, "d", desc="Send data from memory to requestor. May need to send sharer number, too") {
      peek(memQueue_in, MemoryMsg) {
          enqueue(response_out, ResponseMsg, 1) {
              out_msg.addr := address;
              out_msg.Type := CoherenceResponseType:Data;
              out_msg.Sender := machineID;
              out_msg.Destination.add(in_msg.OriginalRequestorMachId);
              out_msg.DataBlk := in_msg.DataBlk;
              out_msg.MessageSize := MessageSizeType:Data;

              DPRINTF(RubySlicc, "sendDataToReq Msg? %s", in_msg);
              Entry e := getDirectoryEntry(address);
              // Only need to include acks if we are the owner.
              if (e.Owner.isElement(in_msg.OriginalRequestorMachId)) {
                  out_msg.Acks := e.Sharers.count();
              } else {
                  out_msg.Acks := 0;
              }
              assert(out_msg.Acks >= 0);
          }
      }
  }

  action(sendPutAck, "a", desc="Send the put ack") {
      peek(request_in, RequestMsg) {
          enqueue(forward_out, RequestMsg, 1) {
              out_msg.addr := address;
              out_msg.Type := CoherenceRequestType:PutAck;
              out_msg.Requestor := machineID;
              out_msg.Destination.add(in_msg.Requestor);
              out_msg.MessageSize := MessageSizeType:Control;
          }
      }
  }
  action(popResponseQueue, "pR", desc="Pop the response queue") {
    response_in.dequeue(clockEdge());
  }

  action(popRequestQueue, "pQ", desc="Pop the request queue") {
      request_in.dequeue(clockEdge());
  }

  action(popMemQueue, "pM", desc="Pop the memory queue") {
      memQueue_in.dequeue(clockEdge());
  }

  action(stall, "z", desc="Stall the incoming request") {
      // Do nothing.
  }
  transition({I, S}, GetS, S_m) {
      sendMemRead;
      addReqToSharers;
      popRequestQueue;
  }

  transition(I, {PutSNotLast, PutSLast, PutMNonOwner}) {
      sendPutAck;
      popRequestQueue;
  }

  transition(S_m, MemData, S) {
      sendDataToReq;
      popMemQueue;
  }

  transition(I, GetM, M_m) {
      sendMemRead;
      setOwner;
      popRequestQueue;
  }

  transition(M_m, MemData, M) {
      sendDataToReq;
      clearSharers; // NOTE: This isn't *required* in some cases.
      popMemQueue;
  }

  transition(S, GetM, M_m) {
      sendMemRead;
      removeReqFromSharers;
      sendInvToSharers;
      setOwner;
      popRequestQueue;
  }

  transition({S, S_D, SS_m, S_m}, {PutSNotLast, PutMNonOwner}) {
      removeReqFromSharers;
      sendPutAck;
      popRequestQueue;
  }

  transition(S, PutSLast, I) { // TODO: To LLCOnly
      removeReqFromSharers;
      sendPutAck;
      popRequestQueue;
  }

  transition(M, GetS, S_D) {
      sendFwdGetS;
      addReqToSharers;
      addOwnerToSharers;
      clearOwner;
      popRequestQueue;
  }

  transition(M, GetM) {
      sendFwdGetM;
      clearOwner;
      setOwner;
      popRequestQueue;
  }

  transition({M, M_m, MI_m}, {PutSNotLast, PutSLast, PutMNonOwner}) {
      sendPutAck;
      popRequestQueue;
  }

  transition(M, PutMOwner, MI_m) {
      // mark clean and create CRE (or Not?), like another event PutMOwnerCreateCRE
      sendDataToMem;
      clearOwner;
      sendPutAck;
      popRequestQueue;
  }

  transition(MI_m, MemAck, I) {
      popMemQueue;
  }

  transition(S_D, {GetS, GetM}) {
      stall;
  }

  transition(S_D, PutSLast) {
      removeReqFromSharers;
      sendPutAck;
      popRequestQueue;
  }

  transition(S_D, Data, SS_m) {
      sendRespDataToMem;
      popResponseQueue;
  }

  transition(SS_m, MemAck, S) {
      popMemQueue;
  }

  // If we get another request for a block that's waiting on memory,
  // stall that request.
  transition({MI_m, SS_m, S_m, M_m}, {GetS, GetM}) {
      stall;
  }
}